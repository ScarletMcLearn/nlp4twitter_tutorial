{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humor\n",
    "\n",
    "Analyzing [this paper](http://delivery.acm.org/10.1145/2670000/2661997/p889-zhang.pdf?ip=93.56.117.176&id=2661997&acc=ACTIVE%20SERVICE&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2EE1FD19F641CD8AF3%2E4D4702B0C3E38B35&CFID=594369919&CFTOKEN=49108395&__acm__=1465800260_832800e3ee6747a7cfe9e7d23ad6294a)\n",
    "\n",
    "how to download tweets, see my stream script.\n",
    "\n",
    "cmu pronunciation dictionary (download using the nltk gui, if you want)\n",
    "\n",
    "http://textfiles.com/humor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'B', u'AA1', u'R', u'B', u'AH0', u'L', u'Z']]\n",
      "[[u'B', u'AA1', u'R', u'B', u'IH0', u'K', u'Y', u'UW2']]\n",
      "[[u'B', u'AA1', u'R', u'B', u'IH0', u'K', u'Y', u'UW2', u'D']]\n",
      "[[u'B', u'AA1', u'R', u'B', u'IH0', u'K', u'Y', u'UW2', u'IH0', u'NG']]\n",
      "[[u'B', u'AA1', u'R', u'B', u'IH0', u'K', u'Y', u'UW2', u'Z']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "arpabet = nltk.corpus.cmudict.dict()\n",
    "for word in ('barbels', 'barbeque', 'barbequed', 'barbequeing', 'barbeques'):\n",
    "    print(arpabet[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "btw, i would ike to draw your attention to this paper: http://www.cs.cmu.edu/~ark/TweetNLP/owoputi+etal.naacl13.pdf, the beginning of the ark system at cmu... it's how, imho, a paper should be written.\n",
    "\n",
    "i also do my part: http://www.inf.udec.cl/~josefuentes/sea2015/... i'm not a phony!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9b3bf35fe68f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSenticnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mconcept_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'love'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/leo/anaconda2/lib/python2.7/site-packages/senticnet/senticnet.pyc\u001b[0m in \u001b[0;36mconcept\u001b[1;34m(self, concept)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"polarity\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparsed_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentics\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparsed_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"semantics\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msemantics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparsed_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/leo/anaconda2/lib/python2.7/site-packages/senticnet/senticnet.pyc\u001b[0m in \u001b[0;36mpolarity\u001b[1;34m(self, concept, parsed_graph)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mparsed_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept_polarity_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparsed_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mURIRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicate_uri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# private methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from senticnet.senticnet import Senticnet\n",
    "sn = Senticnet()\n",
    "\n",
    "concept_info = sn.concept('love')\n",
    "\n",
    "polarity = sn.polarity('love')\n",
    "semantics = sn.semantics('love')\n",
    "sentics = sn.sentics('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happy = swn.senti_synsets('happy', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SentiSynset('happy.a.01')]\n"
     ]
    }
   ],
   "source": [
    "print happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breakdown = swn.senti_synset('happy.a.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<happy.a.01: PosScore=0.875 NegScore=0.0>\n"
     ]
    }
   ],
   "source": [
    "print breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted\n",
    "Regression Trees or GBRT (Friedman, 1999), based on decision trees\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lexico-semantic features alone can recognize humorous tweets among all the tweets with an accuracy of 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "btw, i would like to draw your attention to gensim: https://radimrehurek.com/gensim/, having several scalable algorithms for semantic analysis (word2vec, which johan was interested in) in parallel (distributed), which we will use in our topic detection class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony\n",
    "\n",
    "http://www.lrec-conf.org/proceedings/lrec2014/pdf/231_Paper.pdf\n",
    "\n",
    "Def'n: By the use of irony,\n",
    "people hide the real meaning of a statement saying the opposite\n",
    "of what they mean\n",
    "\n",
    "Corpus: http://users.dsic.upv.es/grupos/nle/?file=kop4.php... it's not accessible, but this has to do more with Twitter's restrictions than it not being available. if we shall ask, we shall get :)\n",
    "\n",
    "We approach the detection of irony as a classification problem\n",
    "applying supervised machine learning methods to the\n",
    "Twitter corpus described in Section 3\n",
    "\n",
    "Also, you can take a look at the http://www.anc.org/, and it's free (http://www.corpusitaliano.it/en/contents/description.html, for a big Italian corpus... is there a better one?)\n",
    "\n",
    "Used the Stanford POS tagger (for Twitter, see http://nlp.stanford.edu/software/tagger.shtml). In particular:\n",
    "\n",
    " - https://gate.ac.uk/wiki/twitter-postagger.html\n",
    " - a python port working in nltk > 2.0, https://github.com/nltk/nltk/blob/master/nltk/tag/stanford.py\n",
    " \n",
    "for the sake of it, check out https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html, parser written in tensorflow\n",
    " \n",
    "For classifiers they avoided those requiring features to be independent\n",
    "(e.g. Naive Bayes) as some of our features are\n",
    "not. Since we approach the problem as a binary decision\n",
    "(deciding if a tweet is ironic or not) we picked a tree-based\n",
    "classifiers: Decision Tree.\n",
    "\n",
    "Features:\n",
    "\n",
    "• Frequency (gap between rare and common words)\n",
    "\n",
    "• Written-Spoken (written-spoken style uses)\n",
    "\n",
    "• Intensity (intensity of adverbs and adjectives)\n",
    "\n",
    "• Structure (length, punctuation, emoticons)\n",
    "\n",
    "• Sentiments (gap between positive and negative terms)\n",
    "\n",
    "• Synonyms (common vs. rare synonyms use)\n",
    "\n",
    "• Ambiguity (measure of possible ambiguities)\n",
    "\n",
    "The features which are more discriminative of ironic style\n",
    "are rarest value, synonym lower, synonym greater gap,\n",
    "and punctuation, suggesting that Frequency, Structure and\n",
    "choice of the Synonym are important aspects to consider\n",
    "for irony detection in tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarcasm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
