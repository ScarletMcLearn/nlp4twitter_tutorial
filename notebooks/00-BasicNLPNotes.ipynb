{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".definition{padding         : 1em;\n",
       "            background-color: Aquamarine;\n",
       "            border: 1px solid blue;}\n",
       ".important{ padding         : 1em;\n",
       "            background-color: red;\n",
       "            border: 1px solid blue;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".definition{padding         : 1em;\n",
    "            background-color: Aquamarine;\n",
    "            border: 1px solid blue;}\n",
    ".important{ padding         : 1em;\n",
    "            background-color: red;\n",
    "            border: 1px solid blue;}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Basic NLP & SA I\n",
    "\n",
    "This lecture has three parts:\n",
    "\n",
    "1. Preliminaries and Introduction to Sentiment Analysis\n",
    "2. Polarity\n",
    "3. Opinion\n",
    "\n",
    "This document contains notes for Part 1. [Polariy](Polarity.ipynb) and [Opinion](Opinion.ipynb) contain notes for Parts 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "\n",
    "### A tweet<br>\n",
    "\n",
    "\n",
    "<div class=\"definition\">\n",
    "**Def'n**: A tweet $t$ is a sequence (string) of Unicode (UTF-8) encoded characters $(c_1,c_2,...,c_n)$, where $|t|=n$ is the length of $t$, and $0 < | t | \\leq 140$,\n",
    "</div>\n",
    "\n",
    "where UTF-8 means encoded in one to four bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mU+0065 LATIN SMALL LETTER E\u001b[0m\r\n",
      "\u001b[32mUTF-8: \u001b[0m65  \u001b[32mUTF-16BE: \u001b[0m0065  \u001b[32mDecimal: \u001b[0m&#101;\r\n",
      "e (E)\r\n",
      "\u001b[32mUppercase: \u001b[0mU+0045\r\n",
      "\u001b[32mCategory: \u001b[0mLl (Letter, Lowercase)\r\n",
      "\u001b[32mBidi: \u001b[0mL (Left-to-Right)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!unicode e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mU+00E9 LATIN SMALL LETTER E WITH ACUTE\u001b[0m\r\n",
      "\u001b[32mUTF-8: \u001b[0mc3 a9  \u001b[32mUTF-16BE: \u001b[0m00e9  \u001b[32mDecimal: \u001b[0m&#233;\r\n",
      "é (É)\r\n",
      "\u001b[32mUppercase: \u001b[0mU+00C9\r\n",
      "\u001b[32mCategory: \u001b[0mLl (Letter, Lowercase)\r\n",
      "\u001b[32mBidi: \u001b[0mL (Left-to-Right)\r\n",
      "\u001b[32mDecomposition: \u001b[0m0065 0301\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!unicode é # combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mU+0301 COMBINING ACUTE ACCENT\u001b[0m\r\n",
      "\u001b[32mUTF-8: \u001b[0mcc 81  \u001b[32mUTF-16BE: \u001b[0m0301  \u001b[32mDecimal: \u001b[0m&#769;\r\n",
      " ́\r\n",
      "\u001b[32mCategory: \u001b[0mMn (Mark, Non-Spacing)\r\n",
      "\u001b[32mBidi: \u001b[0mNSM (Non-Spacing Mark)\r\n",
      "\u001b[32mCombining: \u001b[0m230 (Above)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!unicode -x 301 # diacritical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unicode` is in the Ubuntu repositories. To install: `sudo apt-get install unicode`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`e` is one byte (`0x65`), `é` is either two bytes `0xc3 0xc9` or three `0x65 0xcc 0x81`. See [this](https://twitter.com/leoferres/status/729705274408239105).\n",
    "\n",
    "Twitter's javascript page imposes composition, the API does not. What can we do with information load about this? What's the length of CJK languages?\n",
    "\n",
    "**Note**: We do not deal with a tweet's metadata, which would mean defining a tweet as a k-tuple with the tweet's text being only one member of the tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work out some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = u\"felicidadees!! k t lo pases muy bien!! =)\"\n",
    "t2 = u\"Feeeliiciidaaadeeess !! (:Felicidadesss!!pasatelo genialll :D\"\n",
    "t3 = u\"FeliicCiidaDesS! :D Q tte Lo0 paseS bN! ;) ♥\"\n",
    "t4 = \"FeliicCiidaDesS! :D Q tte Lo0 paseS bN! ;) ♥\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F e l i i c C i i d a D e s S !   : D   Q   t t e   L o 0   p a s e S   b N !   ; )   ♥\n"
     ]
    }
   ],
   "source": [
    "seq =[]\n",
    "for c in t3:\n",
    "    print c,\n",
    "    seq.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 44\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print \"Number of characters: \" + str(len(t3))\n",
    "print len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F e l i i c C i i d a D e s S !   : D   Q   t t e   L o 0   p a s e S   b N !   ; )   � � �\n"
     ]
    }
   ],
   "source": [
    "for c in t4:\n",
    "    print c,\n",
    "    seq.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 46\n"
     ]
    }
   ],
   "source": [
    "print \"Number of characters: \" + str(len(t4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mU+2665 BLACK HEART SUIT\u001b[0m\r\n",
      "\u001b[32mUTF-8: \u001b[0me2 99 a5  \u001b[32mUTF-16BE: \u001b[0m2665  \u001b[32mDecimal: \u001b[0m&#9829;\r\n",
      "♥\r\n",
      "\u001b[32mCategory: \u001b[0mSo (Symbol, Other)\r\n",
      "\u001b[32mBidi: \u001b[0mON (Other Neutrals)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!unicode ♥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main target languages are Spanish, Italian and English. We still need to (at the very least) discard those messages that are not in this language. The following module, available at https://github.com/saffsd/langid.py, trained on 97 languages, does the identification for us. See: M. Lui and T. Baldwin. 2012. langid.py: An off-theshelf language identification tool. In Proc. of ACL. [Architecture paper](http://www.aclweb.org/anthology/P12-3005) and [theory paper](http://www.aclweb.org/anthology/I11-1062) (Cited by 47)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../modules/langid.py/langid')\n",
    "from langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('es', 0.9999693636198159)\n",
      "('pt', 0.8822975698500081)\n",
      "('no', 0.332943138461849)\n"
     ]
    }
   ],
   "source": [
    "print identifier.classify(t1)\n",
    "print identifier.classify(t2)\n",
    "print identifier.classify(t3) # t4 is t3 with no unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('it', 0.9999999999999993)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5 = \"Ho aggiunto un video a una playlist di @YouTube: https://t.co/jrSt4uW17P Joybiza\"\n",
    "identifier.classify(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "byte n-grams where $1\\leq n\\leq 5$, no assumptions about the language. the N most frequent terms for each language are retained in the global feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice how difficult it becomes for `LangID` to identify some of the tweets, given that some of the byte n-grams do not really belong to Spanish (but maybe they do in Portuguese/Norwegian?). This brings us to another pre-processing step: tweet **normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words\n",
    "\n",
    "The def'n above is sufficiently broad to account for any tweet in any (human or machine) language. But we want to know what a tweet is \"communicating\", not what bytes it's composed of. \n",
    "\n",
    "For that, \"split\" a tweet (in the sense above) into composing elements. Some elements are inherent to tweets (at-mentions, hashtags, or RTs), some are more general: words in the language the tweet is written in.\n",
    "\n",
    "Words are notoriously difficult to define. However, for practical reasons, we will have that: <br><br>\n",
    "\n",
    "<div class=\"definition\">\n",
    "**Def'n**: A word $w$ is a sequence of Unicode (UTF-8) encoded characters separated by either a space or a symbol in some predefined set of [punctuation marks](https://en.wikipedia.org/wiki/Punctuation_of_English).\n",
    "</div>\n",
    "\n",
    "We may extend this if we find someone willing to work on CJK languages, which would be awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "In lexical analysis, tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. [from Wikipedia](https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)). For most of our purposes, the simple, regular expression package in NLTK is enough for tokenizing tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'felicidadees', u'!', u'!', u'k', u't', u'lo', u'pases', u'muy', u'bien', u'!', u'!', u'=)']\n",
      "[u'Feeeliiciidaaadeeess', u'!', u'!', u'(:', u'Felicidadesss', u'!', u'!', u'pasatelo', u'genialll', u':D']\n",
      "[u'FeliicCiidaDesS', u'!', u':D', u'Q', u'tte', u'Lo0', u'paseS', u'bN', u'!', u';)', u'\\u2665']\n"
     ]
    }
   ],
   "source": [
    "print tknzr.tokenize(t1)\n",
    "print tknzr.tokenize(t2)\n",
    "print tknzr.tokenize(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'felicidadees', u'!!', u'k', u't', u'lo', u'pases', u'muy', u'bien', u'!!', u'=)']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../modules/ark-twokenize-py')\n",
    "import twokenize as twokenize\n",
    "t = twokenize.tokenize(t1)\n",
    "print t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is tough. Twitter's messages show very noise input, like the ones shown again below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.felicidadees!! k t lo pases muy bien!! =)\n",
      "2.Feeeliiciidaaadeeess !! (:Felicidadesss!!pasatelo genialll :D\n",
      "3.FeliicCiidaDesS! :D Q tte Lo0 paseS bN! ;) ♥\n",
      "4Ho aggiunto un video a una playlist di @YouTube: https://t.co/jrSt4uW17P Joybiza\n"
     ]
    }
   ],
   "source": [
    "print \"1.\" + t1\n",
    "print \"2.\" + t2\n",
    "print \"3.\" + t3\n",
    "print \"4\" + t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the task of text normalisation to be a mapping from “ill-formed” OOV lexical items to their standard lexical forms [[P11-1038.pdf](./papers/P11-1038.pdf)]. Twitter text normalization is not a simple task. It involves restoring capitalization, normalizing weird spelling conventions, among many other processes. However, we do not want to lose information. Thus, although we normalize for furthering the next processes, we keep the original spelling, which also conveys information. For example, the tweet\n",
    "\n",
    "We will be somewhat naïve in normalizing at this point. But I want to introduce the concept of *word* a bit later, so we'll use a simple normalizer based on a list of words. It is called [deflog](https://github.com/sbruno/deflog). Comes from de-fotologging... don't search for flogger :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('../modules/')\n",
    "import libdeflog as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'felicidades', u'!', u'!', u'que', u'te', u'lo', u'pases', u'muy', u'bien', u'!', u'!', u'=)']\n"
     ]
    }
   ],
   "source": [
    "q = [df.desms(df.desmultiplicar(w)) for w in tknzr.tokenize(t1)]\n",
    "print q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we must be careful: by \"doubling up\" letters expresses a sentiment emphasis of sorts. That is the kind of information, we want to maintain! So instead of actually clobbering stuff, we may want to simply **add** information to some structure, like a `json` file. We will use this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-Tagging\n",
    "\n",
    "Finally, there's POS-Tagging, probably the most difficult *linguistic* task in this introductory document.\n",
    "\n",
    "Part of speech tagging is the process of identifying nouns, verbs, adjectives, and other parts of speech in context.\n",
    "\n",
    "There are several POSTaggers. \n",
    "\n",
    "### Stanford\n",
    "\n",
    "1. Download http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip (or latest), see documentation at http://nlp.stanford.edu/software/tagger.shtml\n",
    "1. Make sure you have Java > 8, if not, follow instructions from http://www.webupd8.org/2012/09/install-oracle-java-8-in-ubuntu-via-ppa.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger as POSTagger\n",
    "spanish_postagger = POSTagger('../modules/stanford-postagger-full-2015-04-20/models/spanish.tagger','../modules/stanford-postagger-full-2015-04-20/stanford-postagger.jar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, enough of introductory stuff. We have covered many (although very naïve) pre-processing techniques for the manipulation of Twitter's text field. Now we will work with the tasks that are important to us, and the first is Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'felicidades', u'nc0p000'),\n",
       " (u'!', u'fat'),\n",
       " (u'!', u'fat'),\n",
       " (u'que', u'cs'),\n",
       " (u'te', u'pp000000'),\n",
       " (u'lo', u'pp000000'),\n",
       " (u'pases', u'vmsp000'),\n",
       " (u'muy', u'rg'),\n",
       " (u'bien', u'rg'),\n",
       " (u'!', u'fat'),\n",
       " (u'!', u'fat'),\n",
       " (u'=)', u'aq0000')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_postagger.tag(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felicidadees!! k t lo pases muy bien!! =)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'f', u'nc0n000'),\n",
       " (u'e', u'cc'),\n",
       " (u'l', u'np00000'),\n",
       " (u'i', u'nc0s000'),\n",
       " (u'c', u'np00000'),\n",
       " (u'i', u'nc0s000'),\n",
       " (u'd', u'np00000'),\n",
       " (u'a', u'sp000'),\n",
       " (u'd', u'nc0s000'),\n",
       " (u'e', u'cc'),\n",
       " (u'e', u'cc'),\n",
       " (u's', u'pi000000'),\n",
       " (u'!', u'fat'),\n",
       " (u'!', u'fat'),\n",
       " (u'k', u'np00000'),\n",
       " (u't', u'np00000'),\n",
       " (u'l', u'np00000'),\n",
       " (u'o', u'cc'),\n",
       " (u'p', u'nc0n000'),\n",
       " (u'a', u'sp000'),\n",
       " (u's', u'nc0n000'),\n",
       " (u'e', u'cc'),\n",
       " (u's', u'np00000'),\n",
       " (u'm', u'np00000'),\n",
       " (u'u', u'cc'),\n",
       " (u'y', u'cc'),\n",
       " (u'b', u'fz'),\n",
       " (u'i', u'nc0s000'),\n",
       " (u'e', u'cc'),\n",
       " (u'n', u'nc0n000'),\n",
       " (u'!', u'fat'),\n",
       " (u'!', u'fat'),\n",
       " (u'=', u'f0'),\n",
       " (u')', u'f0')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print t1\n",
    "spanish_postagger.tag(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
